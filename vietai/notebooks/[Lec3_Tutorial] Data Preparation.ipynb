{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Làm việc với dữ liệu\n",
    "\n",
    "\n",
    "*\"Học Máy (Machine Learning) là một lĩnh vực nghiên cứu và xây dựng các giải thuật có khả năng học tự động từ dữ liệu để giải quyết các vấn đề cụ thể\". Từ đó có thể thấy, dữ liệu (data) đóng một vai trò cực kì quan trọng và là yếu tố đầu tiên cần có khi thực hiện một ứng dụng Machine Learning. *\n",
    "\n",
    "*Trong bài thực hành ngay hôm nay, chúng ta sẽ cùng tìm hiểu một số khái niệm và kĩ thuật căn bản khi làm việc với các loại dữ liệu.*\n",
    "\n",
    "\n",
    "### Mục tiểu buổi học\n",
    "- Tìm hiểu các kiểu dữ liệu cơ bản trong Machine Learning\n",
    "- Làm việc với dữ liệu bảng, văn bản và hình ảnh trên Python\n",
    "- Giới thiệu một số phương pháp lưu trữ dữ liệu \n",
    "\n",
    "### Nội dung \n",
    "1 - Dữ liệu có cấu trúc\n",
    "\n",
    "2 - Dữ liệu không có cấu trúc\n",
    "- Dữ liệu văn bản\n",
    "- Dữ liệu hình ảnh\n",
    "\n",
    "3 - Lưu trữ dữ liệu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong Machine Learning, dữ liệu được chia thành 2 nhóm chính, là **dữ liệu có cấu trúc** (Structured Data) và **dữ liệu không có cấu trúc** (Unstructured Data).\n",
    "## 1. Dữ liệu có cấu trúc\n",
    "- Dữ liệu thường biểu diễn ở dạng bảng\n",
    "- Mỗi hàng biểu diễn một **điểm dữ liệu (instance)**\n",
    "- Các cột là các **đặc trưng (features) ** và **nhãn (label)** của dữ liệu đó\n",
    "- Một cách thường dùng để lưu trữ dữ liệu có cấu trúc là lưu trữ ở dạng file **csv** <sup>(1)</sup>\n",
    "\n",
    "Ví dụ: Iris Dataset\n",
    "<img src = \"img/iris.jpg\" width=\"500px\"/>\n",
    "\n",
    "<sup> 1 </sup> **csv** (comma-separated values) là một định dạng file thông dụng để lưu trữ dữ liệu dạng bảng. Mỗi dòng trong file tương ứng với một hàng, dữ liệu trong một dòng mặc định được phân cách bằng dấu **phẩy**, hoặc các kí tự khác được tự định nghĩa (khoảng cách, tab, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
      "['5.1', '3.8', '1.5', '0.3', 'setosa']\n",
      "['5.1', '3.4', '1.5', '0.2', 'setosa']\n",
      "['5.2', '2.7', '3.9', '1.4', 'versicolor']\n",
      "['5.7', '2.6', '3.5', '1.0', 'versicolor']\n",
      "['5.7', '2.8', '4.1', '1.3', 'versicolor']\n",
      "['6.0', '2.2', '5.0', '1.5', 'virginica']\n",
      "['6.9', '3.1', '5.4', '2.1', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Có thể sử dụng thư viện hỗ trợ đọc file csv trong Python\n",
    "# Hoặc tiến hành đọc file như file text thông thường\n",
    "import csv   \n",
    "\n",
    "with open('data/iris.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in list(reader)[::20]:\n",
    "        print (row)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để dễ dàng trong phân tích và xử lý các dữ liệu dạng bảng trên Python, một công cụ thường được sử dụng là **Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width     species\n",
      "62            6.0          2.2           4.0          1.0  versicolor\n",
      "135           7.7          3.0           6.1          2.3   virginica\n",
      "125           7.2          3.2           6.0          1.8   virginica\n",
      "82            5.8          2.7           3.9          1.2  versicolor\n",
      "30            4.8          3.1           1.6          0.2      setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "print (df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một số kĩ thuật tiền xử lý dữ liệu có cấu trúc: \n",
    "- Xử lý dữ liệu bị mất (missing data)\n",
    "- Chuẩn hóa dữ liệu (normalization) \n",
    "- Rời rạc hóa dữ liệu (discretization)\n",
    "- Phát hiện và xử lý dữ liệu ngoại lai (outlier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dữ liệu không có cấu trúc\n",
    "Các loại dữ liệu không có cấu trúc thường gặp:\n",
    "- Dữ liệu văn bản (text)\n",
    "- Dữ liệu hình ảnh (image)\n",
    "- Dữ liệu âm thanh (audio)\n",
    "- Dữ liệu chuỗi thời gian (time series)\n",
    "- etc.\n",
    "\n",
    "Đối với các dữ liệu không có cấu trúc, cần có các phương pháp khác nhau để chuyển đổi dữ liệu thô (raw data) thành dạng vector đặc trưng (feature vector) trước khi áp dụng các giải thuật Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dữ liệu văn bản\n",
    "Mục đích của tiền xử lý dữ liệu văn bản là chuyển đổi các từ/kí tự trong văn bản thành các vector thể hiện đặc trưng của văn bản.\n",
    "\n",
    "Các phương pháp tiền xử lý văn bản thường gặp:\n",
    "\n",
    "#### a. Tokenization: \n",
    "Là quá trình phân tách văn bản thành các từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I am writing a sample text.\"\n",
    "tokenized_text = sample_text.split()\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng thư viện nltk để hỗ trợ xử lý dữ liệu text\n",
    "# https://www.nltk.org/\n",
    "\n",
    "import nltk\n",
    "tokenized_text = nltk.word_tokenize(sample_text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Loại bỏ Stopword: \n",
    "Stopword là các từ phổ biến trong ngôn ngữ và không mang ý nghĩa đặc trưng. Đối với các bài toán về phân loại văn bản, các stopword này sẽ được loại bỏ để giảm kích thước tập từ vựng (vocabulary) cũng như hạn chế nhiễu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopword = stopwords.words('english')\n",
    "print (english_stopword[::5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Stemming\n",
    "Với một số ngôn ngữ như tiếng anh, từ vựng thường được biến đổi về hình thức do quy tắc về ngữ pháp. Đối với các bài toán chỉ quan tâm đến ngữ nghĩa của từ mà bỏ qua cấu trúc ngữ pháp, người ta thường dùng kỹ thuật stemming để đưa các từ về dạng gốc (cats -> cat, playing -> play)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed_text = [ps.stem(word) for word in tokenized_text]\n",
    "print (stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Xác định tập từ vựng (vocabulary)\n",
    "Đối với các bài toán về dữ liệu văn bản, cần xác định một tập từ vựng cố định, là các đặc trưng của văn bản. \n",
    "\n",
    "Tập từ vựng thường là tập hợp các từ xuất hiện trong tập dữ liệu huấn luyện, sau khi đã qua các bước như loại bỏ stopword, stemming, etc. Ngoài ra, để giới hạn kích thước tập từ vựng, ta cũng có thể loại bỏ các từ có tần xuất suất hiện quá thấp (các từ hiếm, nhiễu) hay quá phổ biến (các từ không mang nhiều ý nghĩa phân loại) trong toàn tập dữ liệu huấn luyện.\n",
    "\n",
    "Có thể sử dụng 1 phần từ **< UNK >** (Unknown word) để biểu diễn cho các từ không xuất hiện trong tập từ vựng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"I am playing with text\",\n",
    "    \"It is a cat\",\n",
    "    \"I like cat\"\n",
    "]\n",
    "\n",
    "sample_texts = [nltk.word_tokenize(text) for text in sample_texts]\n",
    "vocab = set(sum(sample_texts, []))\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Chuyển văn bản về dạng vector đặc trưng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "index2word = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    # TODO: Tạo mapping cho tập từ vừng: từ -> chỉ số và ngược lại\n",
    "    #\n",
    "    # word2index : key   - từ thuộc vocab \n",
    "    #               value - chỉ số của từ trong vocab\n",
    "    #               {'am': 0, 'I': 1, 'with': 2, ... } \n",
    "    # index2word : key   - chỉ số của từ trong vocab\n",
    "    #               value - từ thuộc vocab \n",
    "    #               {0: 'am', 1: 'I', 2: 'with', ... }\n",
    "    #\n",
    "    \n",
    "\n",
    "    # END\n",
    "    \n",
    "print (word2index)\n",
    "print (index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(text, word2index):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    index_vector = None\n",
    "    \n",
    "    # TODO: Chuyển dữ liệu text về dạng vector chỉ số\n",
    "    #\n",
    "    # @param    text          văn bản cần xử lý\n",
    "    # @param    word2index    mapping word-index\n",
    "    # @return   index_vector  vector chỉ số của văn bản\n",
    "    # \n",
    "    # Ex: text = \"a cat playing with a cat\"\n",
    "    #     word2index = {'am': 0, 'I': 1, 'with': 2, ...} \n",
    "    #     index_vector = [9, 7, 6, 2, 9, 7]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # END\n",
    "    \n",
    "    return index_vector\n",
    "\n",
    "text = \"a cat playing with a cat\"\n",
    "print (text)\n",
    "\n",
    "index_vector = to_index(text, word2index)\n",
    "print (index_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot Vector\n",
    "\n",
    "One-hot vector là vector có tất cả các giá trị bằng 0 và một giá trị duy nhất bằng 1. Vị trí có giá trị bằng 1 chính là giá trị integer mà vector đó biểu diễn. \n",
    "\n",
    "One-hot vector thường đường sử dụng để biểu diễn các biến kiểu phân loại (categorical variable) như các nhãn (label) của bài toán phân loại (classification). Trong các bài toán trên dữ liệu văn bản, One-hot vector cũng được sử đụng để biểu diễn các từ, thay cho giá trị index được biểu diễn ở trên.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_onehot(index_vector, vocab):\n",
    "    \n",
    "    # Mỗi onehot vector sẽ có độ dài bằng vocab_size\n",
    "    vocab_size = len(vocab)\n",
    "    onehot_vectors = None\n",
    "    \n",
    "    # TODO: Chuyển giá trị chỉ số về giá trị vector\n",
    "    # \n",
    "    # @param    index_vector    vector chỉ số của văn bản\n",
    "    # @param    vocab           tập từ vựng\n",
    "    # @return   onehot_vectors  vector đặc trưng dạng one-hot\n",
    "    #\n",
    "    # Ex: index_vector = [9, 7, 6, 2, 9, 7] \n",
    "    #     vocab = ['I', 'am', 'write', 'a', 'sampl', 'text', '.']\n",
    "    #     onehot_vectors = [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "    #                       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "    #                       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "    #                       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "    #                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "    #                       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]\n",
    "    \n",
    "    \n",
    "    # END\n",
    "    \n",
    "    return onehot_vectors\n",
    "    \n",
    "to_onehot(index_vector, vocab)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mô hình bag-of-word\n",
    "\n",
    "Đặc điểm mô hình Bag-of-word:\n",
    "- Coi văn bản là tập hợp của các từ \n",
    "- Không quan tâm đến vị trí xuất hiện của từ trong văn bản\n",
    "- Sử dụng các phương pháp \"đếm\" <sup>2</sup> để tạo ra các vector đặc trưng cho văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_count_vector(index_vector, vocab):\n",
    "    \n",
    "    # Mỗi vector đặc trưng sẽ có độ dài bằng vocab_size\n",
    "    vocab_size = len(vocab)\n",
    "    count_vector = None\n",
    "    \n",
    "    # TODO: Đếm tần suất xuất hiện của các từ \n",
    "    #\n",
    "    # @param   index_vector\n",
    "    # @param   vocab\n",
    "    # @return  count_vector     vector đặc trưng là tần suất xuất hiện của các từ trong văn bản\n",
    "    #\n",
    "    # Ex:  index_vector = [9, 7, 6, 2, 9, 7]\n",
    "    #      count_vector = [0. 0. 0. 1. 0. 2. 1. 0. 0. 2.]\n",
    "\n",
    "    \n",
    "    # END\n",
    "    \n",
    "    return count_vector\n",
    "    \n",
    "print (to_count_vector(index_vector, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng CountVectorizer từ thư viện scikit-learn\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_texts = [\n",
    "    \"I am playing with text\",\n",
    "    \"It is a cat\",\n",
    "    \"I like cat\"\n",
    "]\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "vectorized_data = count_vect.fit_transform(sample_texts)\n",
    "\n",
    "print (\"vocab:\", count_vect.vocabulary_)\n",
    "print (\"count vectorized format:\\n\",vectorized_data)\n",
    "print (\"array format:\\n\", vectorized_data.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup>2</sup> Có nhiều phương pháp \"đếm\" khác nhau được sử dụng để tạo vector đặc trưng cho dữ liệu văn bản. Một phương pháp thường được sử dụng là TF-IDF. \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhược điểm của mô hình:\n",
    "- Bỏ qua thông tin về vị trí xuất hiện của từ trong câu\n",
    "- Không mang thông tin về ngữ nghĩa của các từ trong câu\n",
    "\n",
    "Hiện nay, với sự phát triển của Deep Learning, những kỹ thuật mới như **Word Embedding**, sử dụng **mạng RNN**, etc., được sử dụng để khắc phục các nhược điểm trên. Các kỹ thuật này sẽ được giới thiệu chi tiết trong các bài giảng tiếp theo của khóa học."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dữ liệu hình ảnh\n",
    "\n",
    "##### a. Vector đặc trưng của ảnh\n",
    "- Hình ảnh được lưu trữ trong máy tính dưới dạng ma trận số, các số này thể hiện thông tin về màu sắc của các pixel trong ảnh. \n",
    "- Đối với định dạng ảnh RGB: kích thước ma trận gồm chiều dài, chiều rộng và chiều sâu, giá trị mỗi số trong ma trận nằm trong phạm vi [0-255] và thể hiện độ sáng của pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ sử dụng thư viện Open-CV cho xử lý ảnh trong Python\n",
    "# https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n",
    "import cv2\n",
    "image = cv2.imread(\"img/cat.jpg\")\n",
    "\n",
    "print (\"shape =\", image.shape)\n",
    "print (image[:, :, 2])\n",
    "\n",
    "\n",
    "# Sử dụng thư viện Matplotlib để hiển thị ảnh trong Python\n",
    "# https://matplotlib.org/\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.subplots()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Một số kỹ thuật tiền xử lý ảnh\n",
    "- Khi làm việc với dữ liệu ảnh trong Machine Learning/Deep Learning, kích thước của ảnh thường là cố định. Cần thay đổi kích thước ảnh đầu vào (resize hoặc crop ảnh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay đổi kích thước ảnh\n",
    "resized_image = cv2.resize(image, (15, 15)) \n",
    "print (\"shape =\", resized_image.shape)\n",
    "print (resized_image[:, :, 2])\n",
    "\n",
    "plt.subplots()\n",
    "plt.imshow(resized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi không quan tâm đến màu sắc của ảnh, có thể chuyển ảnh thành dạng ảnh xám (greyscale), lúc này, kích thước của ma trận chỉ gồm 2 giá trị là chiều dài và chiều rộng ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển sang ảnh xám\n",
    "greyscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "print (\"shape =\", greyscale_image.shape)\n",
    "print (greyscale_image[:, :])\n",
    "\n",
    "plt.subplots()\n",
    "plt.imshow(greyscale_image,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để làm giàu cho tập dữ liệu (data augmentation), có thể sử dụng một số kỹ thuật như lật ảnh, xoay ảnh, hay dùng cách lớp mặt nạ (mask) để tạo ra các điểm dữ liệu mới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Lật ảnh\n",
    "horizontal_img = cv2.flip(image, 0 )\n",
    "plt.subplot(141),plt.imshow(horizontal_img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "vertical_img = cv2.flip(image, 1 )\n",
    "plt.subplot(142),plt.imshow(vertical_img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "both_img = cv2.flip(image, -1 )\n",
    "plt.subplot(143),plt.imshow(both_img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Xoay ảnh\n",
    "angle = 20\n",
    "image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "rot_img = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "plt.subplot(144),plt.imshow(rot_img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Với \"Mask\" là một ma trận cho trước \n",
    "# Sử dụng phép nhân hadamard để áp dụng lớp \"Mask\" lên hình ảnh\n",
    "\n",
    "mask = np.random.random(greyscale_image.shape[:2])\n",
    "masked_image = mask * greyscale_image\n",
    "\n",
    "plt.subplots()\n",
    "plt.imshow(masked_image,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lưu trữ dữ liệu\n",
    "Trong quá trình huấn luyện mô hình, tiền xử lý dữ liệu là một bước làm tốn nhiều thời gian và bộ nhớ. Vì vậy thông thường, ta sẽ xử lý các dữ liệu thô (raw data), chuyển thành dạng vector đặc trưng và lưu trữ các vector này dưới dạng nhị phân để thuận tiện trong việc lưu trữ và sử dụng.\n",
    "\n",
    "#### Python Pickle\n",
    "- Cho phép lưu trữ các cấu trúc dữ liệu trong Python dưới dạng file nhị phân.\n",
    "- Các thao tác làm việc vói Pickle đơn giản và nhanh chóng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/pickle.html\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_1 = {\n",
    "    \"apple\": 1.5,\n",
    "    \"orange\": 5.0,\n",
    "    \"peach\": 0\n",
    "}\n",
    "\n",
    "data_2 = np.random.random(10)\n",
    "\n",
    "# Ghi dữ liệu vào file pickle\n",
    "file_name = \"data/pickle_sample.pkl\"\n",
    "pickleFile = open(file_name, 'wb')\n",
    "pickle.dump(data_1, pickleFile, pickle.HIGHEST_PROTOCOL) # https://docs.python.org/3/library/pickle.html#pickle-protocols\n",
    "pickle.dump(data_2, pickleFile, pickle.HIGHEST_PROTOCOL)\n",
    "pickleFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ file pickle\n",
    "pickleFile = open(file_name, 'rb')\n",
    "data_1 = pickle.load(pickleFile)\n",
    "data_2 = pickle.load(pickleFile)\n",
    "pickleFile.close()\n",
    "print (data_1)\n",
    "print (data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample():\n",
    "    def __init__(self):\n",
    "        self.text = \"test data\"\n",
    "    def write_data(self):\n",
    "        print(self.text)\n",
    "        \n",
    "file_name = \"data/pickle_class.pkl\" \n",
    "\n",
    "# Pickle cho phép lưu trữ dữ liệu là class được tự định nghĩa \n",
    "sample_class = Sample()\n",
    "pickleFile = open(file_name, 'wb')\n",
    "pickle.dump(sample_class, pickleFile, pickle.HIGHEST_PROTOCOL)\n",
    "pickleFile.close()\n",
    "\n",
    "pickleFile = open(file_name, 'rb')\n",
    "sample_class = pickle.load(pickleFile)\n",
    "pickleFile.close()\n",
    "sample_class.write_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class cần được định nghĩa trong chương trình\n",
    "file_name = \"data/pickle_unseen_class.pkl\"\n",
    "pickleFile = open(file_name, 'rb')\n",
    "unseen_class = pickle.load(pickleFile)\n",
    "pickleFile.close()\n",
    "unseen_class.write_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lưu trữ file ở dạng Pickle gặp phải một số hạn chế về kích thước không gian lưu trữ tốc độ truy xuất dữ liệu khi làm việc với dữ liệu lớn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDF5\n",
    "- Định dạng HDF5 được sử dụng để lưu trữ dữ liệu lớn dưới dạng file nhị phân\n",
    "- Dữ liệu được lưu trữ dưới dạng cấu trúc phân cấp (hierarchical structure)\n",
    "- Dễ dàng lưu trữ và làm việc với dữ liệu số, dữ liệu Numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.h5py.org/\n",
    "import h5py\n",
    "\n",
    "file_name = \"data/hdf5_sample.hdf5\"\n",
    "hdf5_file = h5py.File(file_name, \"w\")\n",
    "\n",
    "# Tạo dataset rỗng, kiểu integer, kích thước (5,)\n",
    "dset = hdf5_file.create_dataset(\"mydataset\", (5,), dtype= np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo group\n",
    "group = hdf5_file.create_group(\"mygroup\")\n",
    "group_dset = group.create_dataset(\"group_dset\", (5,), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu được lưu trữ bằng cấu trúc phân cấp (như thư mục)\n",
    "print (hdf5_file.name)\n",
    "print (dset.name)\n",
    "print (group.name)\n",
    "print (group_dset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu trữ giá trị vào data set\n",
    "import numpy as np\n",
    "dset.attrs[\"data\"] = np.random.random((5,5))\n",
    "dset.attrs[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.attrs[\"label\"] = np.zeros(5)\n",
    "dset.attrs[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Đóng file\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ hdf5\n",
    "hdf5_file = h5py.File(file_name, \"r\")\n",
    "for name in hdf5_file:\n",
    "    print (name)\n",
    "print (hdf5_file[\"mydataset\"])\n",
    "print (hdf5_file[\"mygroup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Làm việc với dữ liệu hdf5 tương tự Numpy Array\n",
    "dataset = hdf5_file[\"mydataset\"]\n",
    "print (dataset.attrs[\"data\"])\n",
    "print (dataset.attrs[\"data\"].shape)\n",
    "print (dataset.attrs[\"data\"].dtype)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
